# RandomForest
随机森林新闻分类算法

这是一个用java语言写的基于随机森林的新闻分类算法，实现了对10类新闻（财经，科技，汽车，房产，体育，娱乐，军事，游戏，教育，其它）的分类，没调用到机器学习的库

3.2.1 随机森林
随机森林就是通过集成学习的思想将多棵树集成的一种算法，它的基本单元是决策树，而它的本质属于机器学习的一大分支——集成学习（Ensemble Learning）方法。随机森林的名称中有两个关键词，一个是“随机”，一个就是“森林”。“森林”我们很好理解，一棵叫做树，那么成百上千棵就可以叫做森林了，这样的比喻还是很贴切的，其实这也是随机森林的主要思想--集成思想的体现。其实从直观角度来解释，每棵决策树都是一个分类器（假设现在针对的是分类问题），那么对于一个输入样本，N棵树会有N个分类结果。而随机森林集成了所有的分类投票结果，将投票次数最多的类别指定为最终的输出，这就是一种最简单的 Bagging 思想。
3.2.2 决策树
决策树就是一棵树，随机森林就是由多棵决策树组成的，一棵决策树包含一个根节点、若干个内部结点和若干个叶结点；叶结点对应于决策结果，其他每个结点则对应于一个属性测试；每个结点包含的样本集合根据属性测试的结果被划分到子结点中；根结点包含样本全集，从根结点到每个叶子结点的路径对应了一个判定测试序列。
举个例子：
把新闻数据通过分词去掉停用词得到特征向量

特征向量
type

1
[收盘, 物业,  下跌, 第一, 服务, 控股, 跌幅,收盘价, 港元,  物业, 股, 录得, 上涨, 亿元]
财经

2
[财政部, 发布, 一季度, 财政收支, 情况, 累计, 全国, 公共, 预算, 收入, 亿元, 同比, 增长]
财经

3
[人工智能, 搜索, 计算机, 信息安全, 百度, 人工智能,  AI, 研究, 开发, 用于]
科技

4
[企业, 自主, 创新, 主体, 地位, 作用, 提高, 研发, 积极性, 中国工程院, 院士, 中国农业科学院，亿元]
科技

5
[阿尔特, 阿森纳, 主场, 落败, 示意, 球队, 最佳, 状态, 喜悦, 晋级, 强劲, 对手, 队伍]
体育

6
[英雄, 联盟, 终于, 迎来, 新年, 第一场, 正式, 联赛, LPL, 春季, 赛, 德玛, 西亚, 杯中, 队伍]
游戏

7
[火爆, 卷发, 款, 蓬松, 毛躁, 一种, 耀眼, 卷发, 看似, 纠结, 成, 一束, 发丝, 蓬松, 夸张, 发量]
其它


从所有特征是中选取一个特征来作为节点，把数据分成两份，如下图
对左节点进行划分，选取“人工智能”特征进行划分，把数据4分到左子节点，因为左子节点的数据类型只有科技类一种，无需再划分，所有把它标志为科技类。把数据1、2分到右节点，因为数据类型只有财经类一种，无需再划分，所以把它标志为财经类。

对其它可划分节点进行划分操作


3.2.3特征选取
信息增益
  	在决策树算法的学习过程中，信息增益是特征选择的一个重要指标，它定义为一个特征能够为分类系统带来多少信息，带来的信息越多，说明该特征越重要，相应的信息增益也就越大。 决策树学习的关键在于如何选择最优的划分属性，所谓的最优划分属性，对于二元分类而言，就是尽量使划分的样本属于同一类别，即“纯度”最高的属性。那么如何来度量特征（features）的纯度，这时候就要用到“信息熵（information entropy）”。先来看看信息熵的定义：假如当前样本集D中第k类样本所占的比例为pk， k为类别的总数。则样本集的信息熵为：

信息熵的值越小，则D的纯度越高。而信息增益恰好是：信息熵-条件熵。换句话说，信息增益代表了在一个条件下，信息复杂度（不确定性）减少的程度。用公式表示，其中D表示数据集总数，a表示特征，表示特征a的取值为样本总数，在本分类器取{有，无}两个值。
随机选取特征
随机森林的精髓就在于随机，在训练每个决策树模型选取特征的时候，我们将数据集所有特征按他们的信息增益进行降序排序，从前k个特征进行随机选取，容易发现当我们选择的k过大，我们可能会选择到一些信息增益较低的特征，且这些特征对划分子树意义不大，而当k值过小，这又会导致每个模型的相似度过高，这样不能显示出每个模型的特点，而且k不应该是固定的，他们随着特征集T的大小改变，所有我们选取k=。

3.2.4过拟合处理
过拟合定义
过度拟合(overfitting)的标准定义:给定一个假设空间H,一个假设h属于H,如果存在其他的假设h'属于H,使得在训练样例上h的错误率比h'小,但在整个实例分布上h'比h的错误率小,那么就说假设h过度拟合训练数据。overfittingt是这样一种现象:一个假设在训练数据上能够获得比其他假设更好的拟合,但是在训练数据外的数据集上却不能很好的拟合数据.此时我们就叫这个假设出现了overfitting的现象.
决策树过拟合原因
 在决策树模型搭建中，我们使用的算法对于决策树的生长没有合理的限制和修剪的话，决策树的自由生长有可能每片叶子里只包含单纯的事件数据或非事件数据，可以想象，这种决策树当然可以完美匹配（拟合）训练数据，但是一旦应用到新的业务真实数据时，效果是一塌糊涂。
决策树过拟合解决方法
 剪枝是一个简化过拟合决策树的过程，决策树剪枝有预剪枝和后剪枝两种方法，对比预剪枝和后剪枝，后剪枝决策树通常比预剪枝决策树保留了更多的分支，一般情形下，后剪枝决策树的欠拟合风险小，泛华性能往往也要优于预剪枝决策树。所以我们选择后剪枝方法对决策树进行过拟合处理，后剪枝过程是在构建完全决策树之后进行的，并且要自底向上的对树中的所有非叶结点进行逐一考察，若将该结点对应的子树换为叶结点能够带来泛华性能的提升，则把该子树替换为叶结点。
3.2.5参数调优
在随机森林中，树的数量对整体效果起到了较大的作用，树的数量过小会导致随机森林的效果变差，树的数量过大导致训练时间过长。经测试，随机森林的效果随着树的数量增大而增大，当到了某值（阈值）效果最大，之后效果会下降，且邻近阈值的点效果呈现波动状态。


